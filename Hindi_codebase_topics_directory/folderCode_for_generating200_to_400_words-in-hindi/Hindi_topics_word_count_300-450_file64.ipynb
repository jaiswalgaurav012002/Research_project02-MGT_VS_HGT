{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745e9e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\AppData\\Local\\Temp\\ipykernel_13876\\3429355779.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ:   0%|          | 0/5 [00:00<?, ?it/s]2025-02-02 17:55:58,048 - INFO - Successfully fetched article: पर्वतारोहण\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]2025-02-02 17:55:59,056 - INFO - Successfully fetched article: एडमंड हिलेरी\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ:  40%|████      | 2/5 [00:02<00:03,  1.02s/it]2025-02-02 17:56:00,083 - INFO - Successfully fetched article: भारत तिब्बत सीमा पुलिस\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ:  60%|██████    | 3/5 [00:03<00:02,  1.02s/it]2025-02-02 17:56:01,163 - INFO - Successfully fetched article: भारत में पर्यटन\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ:  80%|████████  | 4/5 [00:04<00:01,  1.04s/it]2025-02-02 17:56:02,186 - INFO - Successfully fetched article: भारतीय थलसेना\n",
      "Searching articles for हिमालय पर्वतारोहण की चुनौतियाँ: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n",
      "2025-02-02 17:56:03,967 - INFO - CSV file has been converted to Excel format.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def search_wikipedia_topics(search_term, max_words, headers, max_articles):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srnamespace\": \"0\",\n",
    "        \"srlimit\": \"1000\",  # Maximum limit per query\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    sroffset = 0  # Offset for pagination\n",
    "    \n",
    "    session = requests_retry_session()\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        params['sroffset'] = sroffset\n",
    "        try:\n",
    "            response = session.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = data.get('query', {}).get('search', [])\n",
    "            if not results:\n",
    "                break  # No more results to fetch\n",
    "            \n",
    "            for result in tqdm(results, desc=f\"Searching articles for {search_term}\"):\n",
    "                title = result['title']\n",
    "                article = fetch_wikipedia_article(title, max_words, headers, search_term)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                \n",
    "                time.sleep(0.5)  # Increased delay to respect rate limits\n",
    "                \n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            sroffset += len(results)  # Move to the next batch of results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error searching for topics with term {search_term}: {e}\")\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_wikipedia_article(title, max_words, headers, search_term):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\",\n",
    "        \"explaintext\": \"\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for page in data['query']['pages'].values():\n",
    "            if 'extract' in page:\n",
    "                content = page['extract']\n",
    "                words = len(content.split())\n",
    "                if words <= max_words:\n",
    "                    logging.info(f\"Successfully fetched article: {title}\")\n",
    "                    return {\n",
    "                        'title': page['title'],\n",
    "                        'content': content,\n",
    "                        'word_count': words,\n",
    "                        'topic': search_term.split()[0]  # Adjust if Hindi topics have spaces\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(f\"Article '{title}' exceeded word count limit\")\n",
    "            else:\n",
    "                logging.warning(f\"No extract found for {title}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "    max_articles = 3000\n",
    "    min_words = 301\n",
    "    max_words = 450\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'MultiTopicArticlesFetcher/1.0 (yourname@example.com)'\n",
    "    }\n",
    "    \n",
    "    # List of new topics in Hindi\n",
    "    topics = [\n",
    "           \"हिमालय पर्वतारोहण की चुनौतियाँ\"\n",
    "    ]\n",
    "    \n",
    "    for topic in topics:\n",
    "        if len(articles) < max_articles:\n",
    "            articles.extend(search_wikipedia_topics(topic, max_words, headers, max_articles - len(articles)))\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'multi_topic_articles_300_to_450_64.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'content', 'word_count', 'topic']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for article in articles[:max_articles]:\n",
    "            writer.writerow(article)\n",
    "            \n",
    "    \n",
    "    # Convert CSV to Excel\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    df.to_excel('multi_topic_articles_300_to_450_64.xlsx', index=False)\n",
    "    logging.info(\"CSV file has been converted to Excel format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d7aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
