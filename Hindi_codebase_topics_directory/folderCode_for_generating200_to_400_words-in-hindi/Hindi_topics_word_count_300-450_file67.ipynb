{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5924e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching articles for लोककथाओं का समाजिक महत्व:   0%|          | 0/1 [00:00<?, ?it/s]2025-02-02 17:59:24,437 - INFO - Successfully fetched article: नारीवाद\n",
      "Searching articles for लोककथाओं का समाजिक महत्व: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "2025-02-02 17:59:26,814 - INFO - CSV file has been converted to Excel format.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def search_wikipedia_topics(search_term, max_words, headers, max_articles):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srnamespace\": \"0\",\n",
    "        \"srlimit\": \"1000\",  # Maximum limit per query\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    sroffset = 0  # Offset for pagination\n",
    "    \n",
    "    session = requests_retry_session()\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        params['sroffset'] = sroffset\n",
    "        try:\n",
    "            response = session.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = data.get('query', {}).get('search', [])\n",
    "            if not results:\n",
    "                break  # No more results to fetch\n",
    "            \n",
    "            for result in tqdm(results, desc=f\"Searching articles for {search_term}\"):\n",
    "                title = result['title']\n",
    "                article = fetch_wikipedia_article(title, max_words, headers, search_term)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                \n",
    "                time.sleep(0.5)  # Increased delay to respect rate limits\n",
    "                \n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            sroffset += len(results)  # Move to the next batch of results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error searching for topics with term {search_term}: {e}\")\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_wikipedia_article(title, max_words, headers, search_term):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\",\n",
    "        \"explaintext\": \"\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for page in data['query']['pages'].values():\n",
    "            if 'extract' in page:\n",
    "                content = page['extract']\n",
    "                words = len(content.split())\n",
    "                if words <= max_words:\n",
    "                    logging.info(f\"Successfully fetched article: {title}\")\n",
    "                    return {\n",
    "                        'title': page['title'],\n",
    "                        'content': content,\n",
    "                        'word_count': words,\n",
    "                        'topic': search_term.split()[0]  # Adjust if Hindi topics have spaces\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(f\"Article '{title}' exceeded word count limit\")\n",
    "            else:\n",
    "                logging.warning(f\"No extract found for {title}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "    max_articles = 3000\n",
    "    min_words = 301\n",
    "    max_words = 450\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'MultiTopicArticlesFetcher/1.0 (yourname@example.com)'\n",
    "    }\n",
    "    \n",
    "    # List of new topics in Hindi\n",
    "    topics = [\n",
    "           \"लोककथाओं का समाजिक महत्व\"\n",
    "    ]\n",
    "    \n",
    "    for topic in topics:\n",
    "        if len(articles) < max_articles:\n",
    "            articles.extend(search_wikipedia_topics(topic, max_words, headers, max_articles - len(articles)))\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'multi_topic_articles_300_to_450_67.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'content', 'word_count', 'topic']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for article in articles[:max_articles]:\n",
    "            writer.writerow(article)\n",
    "            \n",
    "    \n",
    "    # Convert CSV to Excel\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    df.to_excel('multi_topic_articles_300_to_450_67.xlsx', index=False)\n",
    "    logging.info(\"CSV file has been converted to Excel format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
