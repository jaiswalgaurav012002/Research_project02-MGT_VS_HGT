{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\AppData\\Local\\Temp\\ipykernel_11184\\2187830550.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: openpyxl in c:\\users\\gaurav\\appdata\\roaming\\python\\python310\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gaurav\\appdata\\roaming\\python\\python310\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching articles for भौतिकी:   0%|          | 0/500 [00:00<?, ?it/s]2025-01-30 07:48:06,901 - INFO - Successfully fetched article: भौतिक शास्त्र\n",
      "Searching articles for भौतिकी:   0%|          | 1/500 [00:01<08:42,  1.05s/it]2025-01-30 07:48:07,983 - INFO - Successfully fetched article: चिरसम्मत भौतिकी\n",
      "Searching articles for भौतिकी:   0%|          | 2/500 [00:02<08:48,  1.06s/it]2025-01-30 07:48:09,071 - INFO - Successfully fetched article: अनुप्रयुक्त भौतिकी\n",
      "Searching articles for भौतिकी:   1%|          | 3/500 [00:03<08:53,  1.07s/it]2025-01-30 07:48:10,124 - INFO - Successfully fetched article: कण भौतिकी\n",
      "Searching articles for भौतिकी:   1%|          | 4/500 [00:04<08:49,  1.07s/it]2025-01-30 07:48:11,232 - INFO - Successfully fetched article: नाभिकीय भौतिकी\n",
      "Searching articles for भौतिकी:   1%|          | 5/500 [00:05<08:53,  1.08s/it]2025-01-30 07:48:12,363 - INFO - Article 'अभिकलनात्मक भौतिकी' exceeded word count limit\n",
      "Searching articles for भौतिकी:   1%|          | 6/500 [00:06<09:02,  1.10s/it]2025-01-30 07:48:13,480 - INFO - Successfully fetched article: भौतिकी में नोबेल पुरस्कार\n",
      "Searching articles for भौतिकी:   1%|▏         | 7/500 [00:07<09:05,  1.11s/it]2025-01-30 07:48:14,645 - INFO - Successfully fetched article: सैद्धान्तिक भौतिकी\n",
      "Searching articles for भौतिकी:   2%|▏         | 8/500 [00:08<09:13,  1.13s/it]2025-01-30 07:48:15,760 - INFO - Successfully fetched article: साहा नाभिकीय भौतिकी संस्थान\n",
      "Searching articles for भौतिकी:   2%|▏         | 9/500 [00:09<09:11,  1.12s/it]2025-01-30 07:48:17,055 - INFO - Successfully fetched article: सौर भौतिकी\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def search_wikipedia_topics(search_term, max_words, headers, max_articles):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srnamespace\": \"0\",\n",
    "        \"srlimit\": \"1000\",  # Maximum limit per query\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    sroffset = 0  # Offset for pagination\n",
    "    \n",
    "    session = requests_retry_session()\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        params['sroffset'] = sroffset\n",
    "        try:\n",
    "            response = session.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = data.get('query', {}).get('search', [])\n",
    "            if not results:\n",
    "                break  # No more results to fetch\n",
    "            \n",
    "            for result in tqdm(results, desc=f\"Searching articles for {search_term}\"):\n",
    "                title = result['title']\n",
    "                article = fetch_wikipedia_article(title, max_words, headers, search_term)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                \n",
    "                time.sleep(0.5)  # Increased delay to respect rate limits\n",
    "                \n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            sroffset += len(results)  # Move to the next batch of results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error searching for topics with term {search_term}: {e}\")\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_wikipedia_article(title, max_words, headers, search_term):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\",\n",
    "        \"explaintext\": \"\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for page in data['query']['pages'].values():\n",
    "            if 'extract' in page:\n",
    "                content = page['extract']\n",
    "                words = len(content.split())\n",
    "                if words <= max_words:\n",
    "                    logging.info(f\"Successfully fetched article: {title}\")\n",
    "                    return {\n",
    "                        'title': page['title'],\n",
    "                        'content': content,\n",
    "                        'word_count': words,\n",
    "                        'topic': search_term.split()[0]  # Adjust if Hindi topics have spaces\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(f\"Article '{title}' exceeded word count limit\")\n",
    "            else:\n",
    "                logging.warning(f\"No extract found for {title}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "    max_articles = 3000\n",
    "    max_words = 300\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'MultiTopicArticlesFetcher/1.0 (yourname@example.com)'\n",
    "    }\n",
    "    \n",
    "    # List of topics in Hindi\n",
    "    topics = [\n",
    "          \"भौतिकी\"\n",
    "         \n",
    "    ]\n",
    "    \n",
    "    for topic in topics:\n",
    "        if len(articles) < max_articles:\n",
    "            articles.extend(search_wikipedia_topics(topic, max_words, headers, max_articles - len(articles)))\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'multiple-topic-articles_53.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'content', 'word_count', 'topic']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for article in articles[:max_articles]:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    # Convert CSV to Excel\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    df.to_excel('multi_topic_articles_53.xlsx', index=False)\n",
    "    logging.info(\"CSV file has been converted to Excel format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
