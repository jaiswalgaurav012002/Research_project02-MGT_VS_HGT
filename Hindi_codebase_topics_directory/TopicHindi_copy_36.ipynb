{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in c:\\users\\gaurav\\appdata\\roaming\\python\\python310\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gaurav\\appdata\\roaming\\python\\python310\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching articles for भाषाएँ:   0%|          | 0/500 [00:00<?, ?it/s]2025-01-29 17:12:59,041 - INFO - Article 'भाषा' exceeded word count limit\n",
      "Searching articles for भाषाएँ:   0%|          | 1/500 [00:01<10:17,  1.24s/it]2025-01-29 17:13:00,274 - INFO - Successfully fetched article: भारत की भाषाएँ\n",
      "Searching articles for भाषाएँ:   0%|          | 2/500 [00:02<10:14,  1.23s/it]2025-01-29 17:13:01,491 - INFO - Successfully fetched article: दार्दी भाषाएँ\n",
      "Searching articles for भाषाएँ:   1%|          | 3/500 [00:03<10:09,  1.23s/it]2025-01-29 17:13:03,210 - INFO - Article 'पहाड़ी भाषाएँ' exceeded word count limit\n",
      "Searching articles for भाषाएँ:   1%|          | 4/500 [00:05<11:44,  1.42s/it]2025-01-29 17:13:04,445 - INFO - Successfully fetched article: हिन्द-आर्य भाषाएँ\n",
      "Searching articles for भाषाएँ:   1%|          | 5/500 [00:06<11:10,  1.35s/it]2025-01-29 17:13:05,676 - INFO - Successfully fetched article: हिन्द-ईरानी भाषाएँ\n",
      "Searching articles for भाषाएँ:   1%|          | 6/500 [00:07<10:48,  1.31s/it]2025-01-29 17:13:06,911 - INFO - Successfully fetched article: उत्तराखण्ड की भाषाएँ\n",
      "Searching articles for भाषाएँ:   1%|▏         | 7/500 [00:09<10:33,  1.28s/it]2025-01-29 17:13:08,119 - INFO - Successfully fetched article: ऑस्ट्रोनीशियाई भाषाएँ\n",
      "Searching articles for भाषाएँ:   2%|▏         | 8/500 [00:10<10:22,  1.26s/it]2025-01-29 17:13:09,429 - INFO - Successfully fetched article: केल्टी भाषाएँ\n",
      "Searching articles for भाषाएँ:   2%|▏         | 9/500 [00:11<10:26,  1.28s/it]2025-01-29 17:13:10,654 - INFO - Successfully fetched article: स्लावी भाषाएँ\n",
      "Searching articles for भाषाएँ:   2%|▏         | 10/500 [00:12<10:17,  1.26s/it]2025-01-29 17:13:11,911 - INFO - Successfully fetched article: मीन भाषाएँ\n",
      "Searching articles for भाषाएँ:   2%|▏         | 11/500 [00:14<10:15,  1.26s/it]2025-01-29 17:13:13,163 - INFO - Successfully fetched article: निकोबारी भाषाएँ\n",
      "Searching articles for भाषाएँ:   2%|▏         | 12/500 [00:15<10:14,  1.26s/it]2025-01-29 17:13:14,434 - INFO - Successfully fetched article: फ़्रान्सीसी भाषा\n",
      "Searching articles for भाषाएँ:   3%|▎         | 13/500 [00:16<10:14,  1.26s/it]2025-01-29 17:13:15,732 - INFO - Successfully fetched article: नेपाली भाषा\n",
      "Searching articles for भाषाएँ:   3%|▎         | 14/500 [00:17<10:17,  1.27s/it]2025-01-29 17:13:16,938 - INFO - Successfully fetched article: सामी भाषाएँ\n",
      "Searching articles for भाषाएँ:   3%|▎         | 15/500 [00:19<10:07,  1.25s/it]2025-01-29 17:13:18,178 - INFO - Successfully fetched article: ईरानी भाषा परिवार\n",
      "Searching articles for भाषाएँ:   3%|▎         | 15/500 [00:20<10:58,  1.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV file has been converted to Excel format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 120\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m<\u001b[39m max_articles:\n\u001b[1;32m--> 120\u001b[0m         articles\u001b[38;5;241m.\u001b[39mextend(\u001b[43msearch_wikipedia_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_articles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_articles:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m, in \u001b[0;36msearch_wikipedia_topics\u001b[1;34m(search_term, max_words, headers, max_articles)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article:\n\u001b[0;32m     52\u001b[0m     articles\u001b[38;5;241m.\u001b[39mappend(article)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Increased delay to respect rate limits\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_articles:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def search_wikipedia_topics(search_term, max_words, headers, max_articles):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srnamespace\": \"0\",\n",
    "        \"srlimit\": \"1000\",  # Maximum limit per query\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    sroffset = 0  # Offset for pagination\n",
    "    \n",
    "    session = requests_retry_session()\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        params['sroffset'] = sroffset\n",
    "        try:\n",
    "            response = session.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = data.get('query', {}).get('search', [])\n",
    "            if not results:\n",
    "                break  # No more results to fetch\n",
    "            \n",
    "            for result in tqdm(results, desc=f\"Searching articles for {search_term}\"):\n",
    "                title = result['title']\n",
    "                article = fetch_wikipedia_article(title, max_words, headers, search_term)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                \n",
    "                time.sleep(0.7)  # Increased delay to respect rate limits\n",
    "                \n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            sroffset += len(results)  # Move to the next batch of results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error searching for topics with term {search_term}: {e}\")\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_wikipedia_article(title, max_words, headers, search_term):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\",\n",
    "        \"explaintext\": \"\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for page in data['query']['pages'].values():\n",
    "            if 'extract' in page:\n",
    "                content = page['extract']\n",
    "                words = len(content.split())\n",
    "                if words <= max_words:\n",
    "                    logging.info(f\"Successfully fetched article: {title}\")\n",
    "                    return {\n",
    "                        'title': page['title'],\n",
    "                        'content': content,\n",
    "                        'word_count': words,\n",
    "                        'topic': search_term.split()[0]  # Adjust if Hindi topics have spaces\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(f\"Article '{title}' exceeded word count limit\")\n",
    "            else:\n",
    "                logging.warning(f\"No extract found for {title}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "    max_articles = 3000\n",
    "    max_words = 300\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'MultiTopicArticlesFetcher/1.0 (yourname@example.com)'\n",
    "    }\n",
    "    \n",
    "    # List of topics in Hindi\n",
    "    topics = [\n",
    "         \"भाषाएँ\"\n",
    "         \n",
    "    ]\n",
    "    \n",
    "    for topic in topics:\n",
    "        if len(articles) < max_articles:\n",
    "            articles.extend(search_wikipedia_topics(topic, max_words, headers, max_articles - len(articles)))\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'multiple-topic-articles_36.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'content', 'word_count', 'topic']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for article in articles[:max_articles]:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    # Convert CSV to Excel\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    df.to_excel('multi_topic_articles_36.xlsx', index=False)\n",
    "    logging.info(\"CSV file has been converted to Excel format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
