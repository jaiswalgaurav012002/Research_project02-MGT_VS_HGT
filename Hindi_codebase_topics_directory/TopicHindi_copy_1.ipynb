{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\AppData\\Local\\Temp\\ipykernel_8168\\4063778574.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "Searching articles for खेल:   0%|          | 0/500 [00:00<?, ?it/s]2025-01-28 01:54:23,177 - INFO - Article 'खेल' exceeded word count limit\n",
      "Searching articles for खेल:   0%|          | 1/500 [00:02<20:51,  2.51s/it]2025-01-28 01:54:25,666 - INFO - Successfully fetched article: राष्ट्रमण्डल खेल\n",
      "Searching articles for खेल:   0%|          | 2/500 [00:05<20:45,  2.50s/it]2025-01-28 01:54:28,191 - INFO - Article 'ओलम्पिक खेल' exceeded word count limit\n",
      "Searching articles for खेल:   1%|          | 3/500 [00:07<20:48,  2.51s/it]2025-01-28 01:54:30,717 - INFO - Successfully fetched article: 2010 राष्ट्रमण्डल खेल\n",
      "Searching articles for खेल:   1%|          | 4/500 [00:10<20:48,  2.52s/it]2025-01-28 01:54:33,217 - INFO - Successfully fetched article: 2014 राष्ट्रमण्डल खेल\n",
      "Searching articles for खेल:   1%|          | 5/500 [00:12<20:42,  2.51s/it]2025-01-28 01:54:35,712 - INFO - Successfully fetched article: एशियाई खेल\n",
      "Searching articles for खेल:   1%|          | 6/500 [00:15<20:37,  2.51s/it]2025-01-28 01:54:38,224 - INFO - Successfully fetched article: २००६ एशियाई खेल\n",
      "Searching articles for खेल:   1%|▏         | 7/500 [00:17<20:35,  2.51s/it]2025-01-28 01:54:40,727 - INFO - Successfully fetched article: 2010 एशियाई खेल\n",
      "Searching articles for खेल:   2%|▏         | 8/500 [00:20<20:32,  2.50s/it]2025-01-28 01:54:43,237 - INFO - Article 'मेजर ध्यानचंद खेल रत्न पुरस्कार' exceeded word count limit\n",
      "Searching articles for खेल:   2%|▏         | 9/500 [00:22<20:30,  2.51s/it]2025-01-28 01:54:45,765 - INFO - Successfully fetched article: 2014 एशियाई खेल\n",
      "Searching articles for खेल:   2%|▏         | 10/500 [00:25<20:33,  2.52s/it]2025-01-28 01:54:48,291 - INFO - Article 'खेल सिद्धांत' exceeded word count limit\n",
      "Searching articles for खेल:   2%|▏         | 11/500 [00:27<20:32,  2.52s/it]2025-01-28 01:54:50,873 - INFO - Successfully fetched article: 2018 एशियाई खेल\n",
      "Searching articles for खेल:   2%|▏         | 11/500 [00:30<22:22,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m\n\u001b[0;32m    146\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV file has been converted to Excel format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 130\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m<\u001b[39m max_articles:\n\u001b[1;32m--> 130\u001b[0m         articles\u001b[38;5;241m.\u001b[39mextend(\u001b[43msearch_wikipedia_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_articles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_articles:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m, in \u001b[0;36msearch_wikipedia_topics\u001b[1;34m(search_term, max_words, headers, max_articles)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article:\n\u001b[0;32m     62\u001b[0m     articles\u001b[38;5;241m.\u001b[39mappend(article)\n\u001b[1;32m---> 64\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Increased delay to respect rate limits\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_articles:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def search_wikipedia_topics(search_term, max_words, headers, max_articles):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srnamespace\": \"0\",\n",
    "        \"srlimit\": \"1000\",  # Maximum limit per query\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    sroffset = 0  # Offset for pagination\n",
    "    \n",
    "    session = requests_retry_session()\n",
    "    \n",
    "    while len(articles) < max_articles:\n",
    "        params['sroffset'] = sroffset\n",
    "        try:\n",
    "            response = session.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = data.get('query', {}).get('search', [])\n",
    "            if not results:\n",
    "                break  # No more results to fetch\n",
    "            \n",
    "            for result in tqdm(results, desc=f\"Searching articles for {search_term}\"):\n",
    "                title = result['title']\n",
    "                article = fetch_wikipedia_article(title, max_words, headers, search_term)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                \n",
    "                time.sleep(1.0)  # Increased delay to respect rate limits\n",
    "                \n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            sroffset += len(results)  # Move to the next batch of results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error searching for topics with term {search_term}: {e}\")\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "def fetch_wikipedia_article(title, max_words, headers, search_term):\n",
    "    base_url = \"https://hi.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\",\n",
    "        \"explaintext\": \"\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for page in data['query']['pages'].values():\n",
    "            if 'extract' in page:\n",
    "                content = page['extract']\n",
    "                words = len(content.split())\n",
    "                if words <= max_words:\n",
    "                    logging.info(f\"Successfully fetched article: {title}\")\n",
    "                    return {\n",
    "                        'title': page['title'],\n",
    "                        'content': content,\n",
    "                        'word_count': words,\n",
    "                        'topic': search_term.split()[0]  # Adjust if Hindi topics have spaces\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(f\"Article '{title}' exceeded word count limit\")\n",
    "            else:\n",
    "                logging.warning(f\"No extract found for {title}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    articles = []\n",
    "    max_articles = 1500\n",
    "    max_words = 300\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'MultiTopicArticlesFetcher/1.0 (yourname@example.com)'\n",
    "    }\n",
    "    \n",
    "    # List of topics in Hindi\n",
    "    topics = [\n",
    "        \"खेल\", \"राजनीति\", \"भू-राजनीति\", \"रक्षा\", \"सैन्य\", \"शिक्षा\", \"विज्ञान\" \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    for topic in topics:\n",
    "        if len(articles) < max_articles:\n",
    "            articles.extend(search_wikipedia_topics(topic, max_words, headers, max_articles - len(articles)))\n",
    "        if len(articles) >= max_articles:\n",
    "            break\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'multi_topic_articles.csv'\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'content', 'word_count', 'topic']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for article in articles[:max_articles]:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    # Convert CSV to Excel\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    df.to_excel('multi_topic_articles.xlsx', index=False, encoding='utf-8')\n",
    "    logging.info(\"CSV file has been converted to Excel format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
